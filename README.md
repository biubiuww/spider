## 废话连篇


爬虫自给自足

使用Python3完成

**注：爬虫大多具有时效性，所以早期上传的不一定能用**



这个readme我也是写了又删，删了又写。曾经一度不想更新（害，主要是懒）

现在更新这篇也是单纯的因为太闲了。






## 依赖包

有的可能需要以下包，加粗表示必须滴（技术太菜，只能依赖这些包来搞一搞）


- **requests** 

- **Beautifulsoup4**

- pymongo

- fake_UserAgent

- pymysql



## 目录

- **1024**： 数字社区的图片

- **baiduMap**： 简单调用百度地图的api完成区域类的信息检索，需要用到开发账号

- **cmanuf**：机械工业出版社的pdf下载？**烂尾，bug太多，不修了**

- ~~**novel**：盗版小说的爬虫...存储到数据库中~~

- **qicai**：七彩英语（英文原著）的PDF下载

- **umei**： 批量下载图片

- **kuaishou**: 关键词：快手、无水印、解析、下载

- **yasee1**：年久失修代码，现在的网站已经修改了逻辑

- **proxy_pool**：代理池源自[jhao104/proxy_pool](https://github.com/jhao104/proxy_pool/)

- **tuao8**: 一个小姐姐的图片下载爬虫

- **91user:** 传入UID解析视频m3u8播放链接

